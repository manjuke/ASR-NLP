{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "# NeMo's ASR collection - this collections contains complete ASR models and\n",
    "# building blocks (modules) for ASR\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07074741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will download pre-trained QuartzNet15x5 model from NVIDIA's NGC cloud and instantiate it for you\n",
    "quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476facf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./an4/wav/an4_clstk/mgah/cen2-mgah-b.wav']\n",
    "for fname, transcription in zip(files, quartznet.transcribe(paths2audio_files=files)):\n",
    "  print(f\"Audio in {fname} was recognized as: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Manifests for training\n",
    "\n",
    "# --- Building Manifest Files --- #\n",
    "import json\n",
    "\n",
    "# Function to build a manifest\n",
    "def build_manifest(transcripts_path, manifest_path, wav_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                # Lines look like this:\n",
    "                # <s> transcript </s> (fileID)\n",
    "                transcript = line[: line.find('(')-1].lower()\n",
    "                transcript = transcript.replace('<s>', '').replace('</s>', '')\n",
    "                transcript = transcript.strip()\n",
    "\n",
    "                file_id = line[line.find('(')+1 : -2]  # e.g. \"cen4-fash-b\"\n",
    "                audio_path = os.path.join(\n",
    "                    data_dir, wav_path,\n",
    "                    file_id[file_id.find('-')+1 : file_id.rfind('-')],\n",
    "                    file_id + '.wav')\n",
    "\n",
    "                duration = librosa.core.get_duration(filename=audio_path)\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                    \"audio_filepath\": audio_path,\n",
    "                    \"duration\": duration,\n",
    "                    \"text\": transcript\n",
    "                }\n",
    "                json.dump(metadata, fout)\n",
    "                fout.write('\\n')\n",
    "                \n",
    "# Building Manifests\n",
    "print(\"******\")\n",
    "train_transcripts = data_dir + '/an4/etc/an4_train.transcription'\n",
    "train_manifest = data_dir + '/an4/train_manifest.json'\n",
    "if not os.path.isfile(train_manifest):\n",
    "    build_manifest(train_transcripts, train_manifest, 'an4/wav/an4_clstk')\n",
    "    print(\"Training manifest created.\")\n",
    "\n",
    "test_transcripts = data_dir + '/an4/etc/an4_test.transcription'\n",
    "test_manifest = data_dir + '/an4/test_manifest.json'\n",
    "if not os.path.isfile(test_manifest):\n",
    "    build_manifest(test_transcripts, test_manifest, 'an4/wav/an4test_clstk')\n",
    "    print(\"Test manifest created.\")\n",
    "print(\"***Done***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Config Information ---#\n",
    "# Prepare config.yaml based on required settings \n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "except ModuleNotFoundError:\n",
    "    from ruamel_yaml import YAML\n",
    "config_path = './configs/config.yaml'\n",
    "\n",
    "yaml = YAML(typ='safe')\n",
    "with open(config_path) as f:\n",
    "    params = yaml.load(f)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch training ---- this requires GPU\n",
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DictConfig preparation\n",
    "from omegaconf import DictConfig\n",
    "params['model']['train_ds']['manifest_filepath'] = train_manifest\n",
    "params['model']['validation_ds']['manifest_filepath'] = test_manifest\n",
    "first_asr_model = nemo_asr.models.EncDecCTCModel(cfg=DictConfig(params['model']), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fde82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!!!\n",
    "trainer.fit(first_asr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring using tensor board\n",
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir lightning_logs/\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode wav files using model\n",
    "print(first_asr_model.transcribe(paths2audio_files=['./an4/wav/an4_clstk/mgah/cen2-mgah-b.wav',\n",
    "                                                    './an4/wav/an4_clstk/fmjd/cen7-fmjd-b.wav',\n",
    "                                                    './an4/wav/an4_clstk/fmjd/cen8-fmjd-b.wav',\n",
    "                                                    './an4/wav/an4_clstk/fkai/cen8-fkai-b.wav'],\n",
    "                                 batch_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger batch-size = bigger throughput\n",
    "params['model']['validation_ds']['batch_size'] = 16\n",
    "\n",
    "# Setup the test data loader and make sure the model is on GPU\n",
    "first_asr_model.setup_test_data(test_data_config=params['model']['validation_ds'])\n",
    "first_asr_model.cuda()\n",
    "\n",
    "# We will be computing Word Error Rate (WER) metric between our hypothesis and predictions.\n",
    "# WER is computed as numerator/denominator.\n",
    "# We'll gather all the test batches' numerators and denominators.\n",
    "wer_nums = []\n",
    "wer_denoms = []\n",
    "\n",
    "# Loop over all test batches.\n",
    "# Iterating over the model's `test_dataloader` will give us:\n",
    "# (audio_signal, audio_signal_length, transcript_tokens, transcript_length)\n",
    "# See the AudioToCharDataset for more details.\n",
    "for test_batch in first_asr_model.test_dataloader():\n",
    "        test_batch = [x.cuda() for x in test_batch]\n",
    "        targets = test_batch[2]\n",
    "        targets_lengths = test_batch[3]        \n",
    "        log_probs, encoded_len, greedy_predictions = first_asr_model(\n",
    "            input_signal=test_batch[0], input_signal_length=test_batch[1]\n",
    "        )\n",
    "        # Notice the model has a helper object to compute WER\n",
    "        first_asr_model._wer.update(greedy_predictions, targets, targets_lengths)\n",
    "        _, wer_num, wer_denom = first_asr_model._wer.compute()\n",
    "        wer_nums.append(wer_num.detach().cpu().numpy())\n",
    "        wer_denoms.append(wer_denom.detach().cpu().numpy())\n",
    "\n",
    "# We need to sum all numerators and denominators first. Then divide.\n",
    "print(f\"WER = {sum(wer_nums)/sum(wer_denoms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Transfer learning  requires GPU\n",
    "# Check what kind of vocabulary/alphabet the model has right now\n",
    "print(quartznet.decoder.vocabulary)\n",
    "\n",
    "# Let's add \"!\" symbol there. Note that you can (and should!) change the vocabulary\n",
    "# entirely when fine-tuning using a different language.\n",
    "quartznet.change_vocabulary(\n",
    "    new_vocabulary=[\n",
    "        ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "        'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", \"!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  this requires GPU\n",
    "# Use the smaller learning rate we set before\n",
    "quartznet.setup_optimization(optim_config=DictConfig(new_opt))\n",
    "\n",
    "# Point to the data we'll use for fine-tuning as the training set\n",
    "quartznet.setup_training_data(train_data_config=params['model']['train_ds'])\n",
    "\n",
    "# Point to the new validation data for fine-tuning\n",
    "quartznet.setup_validation_data(val_data_config=params['model']['validation_ds'])\n",
    "\n",
    "# And now we can create a PyTorch Lightning trainer and call `fit` again.\n",
    "trainer = pl.Trainer(gpus=[1], max_epochs=2)\n",
    "trainer.fit(quartznet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
